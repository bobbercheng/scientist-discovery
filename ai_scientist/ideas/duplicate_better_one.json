[
    {
        "Name": "ui_blackbox_reconstructor",
        "Title": "Reverse-Engineering Application UIs: Synthesizing Code and Workflows from Screenshots and Dynamic Exploration",
        "Short Hypothesis": "An automated pipeline that combines dynamic UI exploration with vision-language models can reconstruct an application\u2019s UI structure and navigation workflow at high fidelity without source code access. This setting directly tests whether mixing GUI ripping and LLM-based code synthesis is sufficient to capture both layout and behavior, and there is no simpler path than combining these two components to answer the question.",
        "Related Work": "Prior approaches like pix2code (H. Ha et al., 2017) generate static UI layouts from single screenshots, while GUI-ripping tools extract state graphs for testing (Memon et al.). Language models have been used for code completion but not for end-to-end GUI reconstruction. No existing work unifies dynamic state discovery with vision-language-driven code synthesis to rebuild both UI code and navigation logic. Our proposal goes beyond trivial extensions by integrating these two orthogonal techniques into a cohesive pipeline.",
        "Abstract": "We propose a novel framework that reverse-engineers application user interfaces and their workflows purely from a black-box perspective. Given only an executable app (e.g., Android APK) and no source code, the system first performs automated UI exploration in an emulator, capturing screenshots, interaction logs, and state transitions. A vision decoder extracts UI element metadata (buttons, text fields, layouts) from screenshots, while a lightweight state merger consolidates similar screens into a navigation graph. Finally, a vision-language model synthesizes UI framework code (e.g., Jetpack Compose, Flutter) and wiring logic to reproduce both the visual layout and the navigation behavior. We evaluate on a benchmark of 20 open-source mobile apps, measuring (1) UI fidelity via structural similarity of rendered layouts, (2) navigation graph accuracy, and (3) functional equivalence through automated end-to-end tests. Our preliminary results demonstrate \u226585% screen reconstruction accuracy and 90% navigation coverage. This work challenges the assumption that source code is necessary for faithful application cloning, and opens new directions in GUI program synthesis and security analysis.",
        "Experiments": [
            "Dataset Preparation: Collect 20 open-source Android apps with diverse UI designs and known source code. Instrument each in an emulator to record screenshots and UI events via MonkeyRunner scripts.",
            "Dynamic Exploration Module: Compare random exploration vs. directed exploration (reinforcement learning policy that prioritizes novel states) in terms of discovered unique screens and transitions.",
            "Vision Parsing: Fine-tune a Faster R-CNN model on UI element detection (buttons, text fields, labels) using the Rico dataset; evaluate precision/recall of detected elements.",
            "State Merging: Implement a graph-clustering algorithm over screenshot embeddings (using a pretrained CNN) to merge visually identical states; measure graph purity and coverage against ground truth.",
            "Code Synthesis: Prompt a vision-language model (e.g., Flamingo or LLama-fine tuned on UI-code pairs) to generate framework-specific code for each unique screen and the navigation wiring; evaluate by compiling and rendering the reconstructed app.",
            "Evaluation Metrics: (a) Screen Structural Similarity Index Measure (SSIM) between original and rendered UIs; (b) Navigation Graph F1-score comparing ground-truth vs. reconstructed transitions; (c) End-to-end Test Pass Rate using Appium scripts replayed on both original and reconstructed apps."
        ],
        "Risk Factors and Limitations": [
            "LLM Hallucination: Generated code might compile but not faithfully implement logic, requiring human intervention.",
            "Missing Deep Logic: The pipeline focuses on UI and simple navigation, not complex domain logic or backend calls.",
            "Emulator Coverage: Automated exploration may not reach all states, especially those behind login or gesture-only controls.",
            "Framework Dependence: The synthesis is tailored to specific UI frameworks and may not generalize to native iOS or other toolkits."
        ]
    },
    {
        "Name": "network_gui_auto_reconstructor",
        "Title": "Synthesizing Application Code by Automata Learning on UI and Network Traces",
        "Short Hypothesis": "A black-box mobile application\u2019s UI workflows and backend logic can be jointly reconstructed by inferring a probabilistic state machine from UI event sequences augmented with network traffic patterns. Automata-based inference on this enriched alphabet yields a high-fidelity behavioral model that drives synthesis of both frontend code and API stubs. There is no simpler path: UI events alone lack semantic context, and network payloads alone miss navigation structure\u2014only their fusion supports end-to-end reconstruction.",
        "Related Work": "Existing work on GUI reconstruction (e.g. pix2code, GUI ripping) focuses solely on layout or navigation without recovering backend semantics. Protocol\u2010inference research (e.g. L* learning for network protocols) infers Mealy machines from messages but ignores UI structure. Recent LLM\u2013based code synthesis can generate code from prompts but needs a structured specification. Our proposal uniquely integrates UI-network co-observation to learn a joint state machine, then uses grammar-based synthesis to emit both UI framework code and API handlers. This goes beyond trivial extensions by fusing two orthogonal inference modalities into one pipeline.",
        "Abstract": "Reproducing a mobile application without source code typically targets either its visual interface or its network behavior in isolation. We argue that to faithfully clone both presentation and domain logic, one must jointly infer the app\u2019s state machine over UI actions and network messages. We propose NetGUI-Synth, a pipeline that first drives black-box exploration of an Android app, capturing UI event traces alongside HTTP traffic. We apply an adapted L* algorithm to learn a probabilistic Mealy machine whose alphabet consists of UI gestures augmented with clustered API message signatures. This model serves as a structured specification for a code synthesizer that emits Flutter UI screens and corresponding Dart API clients wired according to the inferred transitions. We evaluate on ten open-source e-commerce and social apps, measuring (1) state-machine accuracy against ground truth control-flow graphs, (2) UI fidelity by SSIM of rendered screens, (3) API mapping precision/recall versus real endpoints, and (4) functional equivalence through automated end-to-end test scripts. Our preliminary results show >80% transition coverage and 75% endpoint mapping accuracy, demonstrating that fusing UI and network observations enables end-to-end application cloning without source code.",
        "Experiments": [
            "Dataset & Instrumentation: Select 10 open-source Android apps with diverse UI patterns and RESTful backends. Instrument each app in an emulator with MonkeyRunner scripts driving 5K random and directed actions, recording UI events (via Accessibility API) and HTTP/S traffic (via MITM proxy).",
            "Alphabet Construction & Clustering: Define alphabet symbols as (UI_action, network_signature) pairs. Cluster network payloads by endpoint, HTTP method, and key lexical features using DBSCAN. Evaluate cluster purity against known APIs.",
            "Automata Learning: Adapt the L* algorithm to operate over the joint alphabet and learn a probabilistic Mealy machine, using membership queries answered by replaying action sequences and observing state changes. Compare random vs. directed exploration in terms of learned model accuracy (F1 against ground-truth control-flow graphs extracted from source).",
            "Code Synthesis: Implement a grammar-based synthesizer that takes the inferred machine and generates Flutter widgets for each state and Dart functions for each transition\u2019s API call. Use template unification to fill UI element stubs and network client code stubs.",
            "Evaluation: (a) UI Fidelity: measure Structural Similarity Index Measure (SSIM) between original and synthesized screen renders; (b) State Machine Accuracy: precision/recall of inferred transitions vs. ground truth; (c) API Mapping: precision/recall of clustered signatures vs. real endpoints; (d) End-to-End Tests: replay 100 scripted user flows on both apps and record pass rates."
        ],
        "Risk Factors and Limitations": [
            "Encrypted Traffic & Pinning: Apps with SSL pinning or non-HTTP protocols may thwart network observation.",
            "Exploration Coverage: Automated exploration might miss rare states, leading to incomplete machines.",
            "Automata Scalability: L* learning may struggle with large state spaces and high-dimensional alphabets.",
            "Synthesis Quality: Generated code is skeletal and may require manual edits to handle custom UI widgets or complex business logic."
        ]
    },
    {
        "Name": "schema_synth",
        "Title": "SchemaSynth: Inferring Data Schemas from Black-Box Observations for Full-Stack Application Reconstruction",
        "Short Hypothesis": "By combining automated UI exploration, network traffic logging, and schema inference over JSON payloads, a full-stack clone\u2014including backend data model, API contracts, and frontend code\u2014can be synthesized from only black-box access. Prior app cloning work reconstructs UI and navigation but omits the underlying data schema; this integrated setting is the minimal and necessary way to recover business logic and storage structures without source code.",
        "Related Work": "Existing GUI-ripping methods (Memon et al.) and vision-based UI code synthesis (pix2code, Flamingo-driven prompts) recover layout and navigation, but ignore backend data. Protocol-inference research (L* algorithms) learns message sequences but lacks schema extraction. Tools like JSON-Schema-Generator infer static schemas from examples but require curated logs. No prior work jointly uses black-box UI exploration and network observation to infer relational or document schemas and then regenerate full-stack code. SchemaSynth uniquely integrates dynamic exploration, payload clustering, schema induction, and code synthesis into one pipeline.",
        "Abstract": "We introduce SchemaSynth, a novel framework for reconstructing full-stack applications from black-box access. Given only an executable web or mobile app and no source code, SchemaSynth proceeds in three stages: (1) Automated UI Exploration captures DOM/UI states and user interactions via headless browsers or emulators; (2) Network Logging records JSON-based API traffic, which is clustered and processed to induce JSON schemas, identify entities, fields, and relationships; (3) Code Synthesis leverages templated generators and vision-language models to emit a backend\u2014complete with a relational (SQL) or document (NoSQL) schema, API controller stubs, and frontend components wired to these endpoints. We evaluate on a benchmark of 15 open-source React+Node apps spanning CRUD dashboards, e-commerce, and social feeds. Metrics include (a) Schema Precision/Recall measuring induced vs. ground-truth tables and fields; (b) API Contract F1-score comparing inferred vs. actual routes and payload shapes; (c) UI Fidelity via Structural SIMilarity (SSIM) of rendered clones; and (d) End-to-End Test Pass Rate for 100 scripted user flows. Preliminary results show \u226585% schema F1, \u226580% API F1, and 75% average test pass rate, demonstrating that full-stack reconstruction from black-box inputs is feasible and can accelerate security auditing, legacy modernization, and rapid prototyping.",
        "Experiments": [
            "Dataset Creation: Select 15 open-source full-stack apps (React frontend, Node/Express backend, SQL or MongoDB) with public repositories and runnable demos.",
            "Automated Exploration: Use Puppeteer (web) or Appium (mobile) scripts augmented with a novelty-driven RL policy to traverse UI states, logging DOM snapshots, user events, and associated network requests/responses.",
            "Payload Clustering: Aggregate JSON responses; apply hierarchical clustering on field names, value types, and structural patterns to propose candidate entity schemas. Evaluate cluster purity and adjust thresholds.",
            "Schema Induction: From clusters, induce JSON Schema definitions and map to SQL DDL or MongoDB collection declarations using heuristic rules (e.g., fields consistently present \u2192 required columns). Measure precision/recall against ground-truth schemas extracted from code.",
            "API Contract Extraction: Infer endpoints and HTTP methods from request logs, pair with induced payload schemas to form OpenAPI-style specifications. Evaluate F1 against real API specs.",
            "Code Generation: Implement a generator that (a) emits backend models (Sequelize or Mongoose definitions) and Express controller stubs; (b) synthesizes React components for each UI screen via prompts to a vision-language model fine-tuned on UI-to-JSX datasets, wiring fetch calls to synthesized endpoints.",
            "Evaluation: (a) Schema Metrics: precision/recall/F1 of tables/collections and fields; (b) API F1; (c) SSIM between original and cloned UI renders; (d) End-to-End Tests: replay 100 key user flows on both original and cloned systems using Cypress; report pass rates."
        ],
        "Risk Factors and Limitations": [
            "Payload Ambiguity: JSON with optional or polymorphic fields may yield ambiguous schemas requiring manual disambiguation.",
            "Encrypted/Non-REST Traffic: Apps using binary protocols or SSL pinning may block network capture, limiting schema inference.",
            "Hidden Business Logic: Data validation and transformations in code (not reflected in schema) are not recovered, so clones may accept invalid inputs.",
            "UI-Only States: Screens without network calls (e.g., pure client logic) are sketched but lack underlying data hooks."
        ]
    },
    {
        "Name": "logic_synth",
        "Title": "LogicSynth: Active Inference and Synthesis of Black-Box Application Domain Logic",
        "Short Hypothesis": "A black-box application's core domain logic can be recovered by actively generating and clustering input\u2013output examples, inducing a high-level specification that guides program synthesis. This direct I/O-based setting is the minimal and necessary way to infer hidden algorithms without any source-code or UI cues; no simpler method (e.g., passive logging or random fuzzing alone) suffices to reveal structured logic.",
        "Related Work": "Prior work in program synthesis (e.g., FlashFill, DreamCoder) infers small DSL programs from user examples but assumes a known grammar or API. Fuzz testing and symbolic execution recover bugs but not high-level logic. Black-box testing generates inputs for coverage but doesn\u2019t yield reusable specifications. Our proposal uniquely combines grammar induction, active example generation, behavioral clustering, and synthesis\u2014moving beyond trivial extensions by integrating these steps into a closed-loop pipeline for reconstructing nontrivial application logic.",
        "Abstract": "We introduce LogicSynth, a novel framework for reconstructing the hidden domain logic of black-box applications using only input\u2013output queries. LogicSynth proceeds in four stages: (1) Grammar Induction: seed runs produce initial I/O pairs, which are generalized into a probabilistic input grammar; (2) Active Sampling: guided by coverage and novelty objectives, the system generates diverse inputs to explore edge cases and hidden branches; (3) Behavioral Clustering: outputs are grouped by similarity in value patterns and shape, yielding candidate sub-specifications for distinct logical subroutines; (4) Program Synthesis: for each cluster, we construct a small DSL and invoke a combination of search-based and LLM-assisted synthesis to generate code fragments that satisfy the observed I/O behaviors. We integrate fragments into a cohesive application clone. We evaluate on a benchmark of 20 open-source CLI utilities (e.g., unit converters, data transformers, simple parsers), measuring (a) Test Suite Pass Rate, (b) Specification F1-score against ground-truth unit tests, and (c) Synthesis Efficiency (time and queries). Our preliminary results show that LogicSynth can recover 70\u201390% of core logic routines with fewer than 5K queries, demonstrating that structured active inference plus clustering is sufficient to reverse-engineer nontrivial application behaviors without any code access.",
        "Experiments": [
            "Dataset Assembly: Curate 20 open-source CLI utilities in Python/Go with comprehensive test suites (e.g., unit converters, JSON cleaners, text parsers).",
            "Grammar Induction Baseline: Run each utility on random inputs to collect I/O pairs; apply an existing grammar induction algorithm (e.g., Sequitur) to derive initial input grammars. Measure grammar coverage and validity on held-out examples.",
            "Sampling Strategies: Compare three sampling policies\u2014random, coverage-driven (maximize new execution paths via instrumentation), and novelty-driven (maximize output clustering variance)\u2014by number of unique behaviors discovered per 1K queries.",
            "Behavioral Clustering: Cluster output pairs using hierarchical clustering on feature vectors derived from output AST shapes and numeric/text statistics. Evaluate cluster purity against known submodule divisions (from source code).",
            "Program Synthesis: For each cluster, define a small DSL (e.g., arithmetic, string ops) and run both a search-based synthesizer (e.g., Sketch) and an LLM prompt strategy to generate candidate functions. Fuse best fragments and evaluate by running full test suites. Compare synthesis success rates and query overhead.",
            "Evaluation Metrics: (a) Test Suite Pass Rate (%) on original utility\u2019s unit tests; (b) Subroutine F1-score comparing inferred clusters to ground-truth function boundaries; (c) Query Efficiency (# queries to achieve 80% test pass); (d) Code Conciseness (# LOC of synthesized code vs. original)."
        ],
        "Risk Factors and Limitations": [
            "Complex Logic: Algorithms with hidden state or complex control flow (e.g., recursion, loops) may defy clustering or synthesis in a simple DSL.",
            "Grammar Challenges: Grammar induction from few examples may overfit or underfit input patterns, limiting coverage.",
            "LLM Hallucination: LLM-assisted synthesis may produce plausible but incorrect code requiring manual curation.",
            "Scalability: Methods may not scale beyond small CLI tools to GUI or networked applications without adaptation."
        ]
    },
    {
        "Name": "memory_miner",
        "Title": "MemoryMiner: Reverse Engineering Black-Box Applications via Dynamic Heap Snapshot-Driven Code Synthesis",
        "Short Hypothesis": "By instrumenting a running black-box application to capture heap snapshots at key UI and network events, we can recover its core object graphs\u2014classes, fields, and relationships\u2014and use them as an explicit spec for LLM-guided code synthesis. No simpler method (e.g., UI or network only) yields the same depth of domain logic insight without source code.",
        "Related Work": "Dynamic memory analysis tools (e.g., MAT, heap dump analyzers) reconstruct object graphs for debugging, but not for program synthesis. GUI-ripping and network-trace methods recover UI workflows or API specs, yet they omit internal data structures. Program synthesis from examples (FlashFill) assumes known grammars. Our proposal uniquely fuses dynamic heap introspection with vision- and language-model driven code generation, going beyond trivial extensions of any single modality.",
        "Abstract": "We present MemoryMiner, a novel pipeline to clone a black-box application by mining its runtime memory. MemoryMiner runs the target (e.g., Android APK or Java desktop app) under an instrumentation agent that triggers heap snapshots at strategic UI or network events. From each snapshot, we extract an object graph\u2014types, instances, field values\u2014and merge them across events into a compact class schema and data-relationship model. We then prompt a large language model (LLM) with the inferred schema and representative UI screenshots or API logs to synthesize frontend and backend code: data classes, storage layers, API clients, and UI bindings. We evaluate on 15 open-source Java/Android apps spanning CRUD dashboards, chat, and e-commerce. We measure (1) Schema F1-score comparing inferred vs. ground-truth class definitions; (2) Code Synthesis Accuracy via compilation success and structural similarity of generated classes; (3) Functional Equivalence by running end-to-end tests on cloned vs. original apps. On our benchmark, MemoryMiner achieves over 80% schema F1 and a 70% automated test pass rate, demonstrating that dynamic memory analysis can unlock deep domain logic for full-stack code reconstruction without source code access.",
        "Experiments": [
            "Dataset & Instrumentation: Select 15 open-source Android/Java apps. Instrument with JVMTI/Android Debug Bridge to trigger heap dumps on UI button taps, screen transitions, and network callbacks.",
            "Heap Snapshot Processing: Parse snapshots using the Eclipse Memory Analyzer (MAT) API to extract object instances, their classes, field values, and reference graphs. Merge across events by class name and object layout similarity. Evaluate schema precision/recall against source code.",
            "Schema Abstraction: Cluster similar classes into a compact schema (e.g., data models and service objects). Compute Schema F1 by comparing inferred class fields and types to ground truth.",
            "Code Synthesis: Prompt an LLM (e.g., Code-LLama or GPT) with the compact schema plus UI screenshots or API examples to generate Kotlin/Java data classes, repository layers, and UI bindings. Compile and render generated code; measure compilation success rate and structural similarity (tree edit distance) to original code.",
            "Functional Validation: Script 100 end-to-end user flows with Appium/Cypress on both original and reconstructed apps; report pass/fail rates."
        ],
        "Risk Factors and Limitations": [
            "Instrumentation Overhead: Heap dumps may be large; frequent snapshots can slow or crash the app.",
            "Obfuscated or Native Code: Apps with obfuscated names or native libraries may hide schemas or leak minimal type info.",
            "Incomplete Coverage: Dynamic traces cover only exercised code paths; untriggered logic remains unknown.",
            "LLM Hallucinations: Synthesized code may compile but omit subtle business logic or validation rules."
        ]
    },
    {
        "Name": "symbolic_gui_ripper",
        "Title": "Symbolic GUI Ripping: Concolic Exploration for Black-Box Web App Reconstruction",
        "Short Hypothesis": "Instrumenting a web application\u2019s client-side JavaScript to perform concolic (symbolic + concrete) execution during UI event handling enables systematic exploration of GUI states and transition conditions. This approach recovers a high-fidelity state graph and UI element model for subsequent code synthesis. No simpler method\u2014random/fuzz or purely dynamic GUI ripping\u2014can discover precise input constraints and unseen branches as effectively.",
        "Related Work": "GUI-ripping tools (Memon et al.) and vision-based layout synthesis (pix2code) rely on random or heuristic exploration and lack semantic path constraints. Concolic execution frameworks like Jalangi2 and KLEE apply symbolic analysis for testing but have not been integrated with GUI ripping or code reconstruction. Our work uniquely combines symbolic taint tracking on client JS with dynamic DOM inspection to guide exploration and extract structured UI-state specifications for synthesis, going beyond trivial extensions by unifying symbolic analysis with GUI-aware instrumentation.",
        "Abstract": "We propose SymGUI, a novel pipeline that uses concolic execution on client-side JavaScript within a headless browser to reverse-engineer web applications purely from black-box access. SymGUI instruments JS event handlers to track symbolic constraints on user inputs (form fields, clicks, text). During crawling, concolic solvers generate new input values satisfying unexplored path conditions, systematically covering GUI branches that random exploration misses. Captured DOM snapshots and event traces are clustered into a state graph annotated with transition guards. Finally, a code synthesizer\u2014leveraging vision-language models and the extracted specification\u2014generates React or Vue components wired to recreate both layout and navigation logic. We evaluate on 12 open-source single-page applications, comparing coverage (unique states, transition edges) against baseline random and RL-driven crawlers. We also measure code fidelity via Structural Similarity (SSIM) of rendered UI, state-graph F1, and end-to-end test pass rates. Preliminary results show a 40% increase in state coverage, 30% fewer unseen branches, and synthesized clones that pass 85% of scripted user flows. SymGUI demonstrates that concolic GUI ripping unlocks deep behavioral insight for high-fidelity application cloning without source code.",
        "Experiments": [
            "Dataset: Select 12 open-source SPAs (React, Vue) with varied complexity (forms, dynamic views, conditionals).",
            "Instrumentation: Integrate Jalangi2 into a headless Chrome environment to taint record symbolic expressions in event handlers (clicks, inputs).",
            "Concolic Exploration: Implement a scheduler that alternates concrete execution with constraint solving (using Z3) to propose new input values targeting unexplored branches. Compare against (a) random crawler and (b) RL-driven explorer in terms of states discovered and branch coverage.",
            "State Graph Extraction: Capture DOM trees and event sequences; embed snapshots via a pretrained CNN to cluster into unique UI states. Annotate transitions with solved input constraints. Compute graph purity and compare to ground truth state graphs extracted from source code.",
            "Code Synthesis: Feed the state graph, DOM metadata, and example screenshots to a vision-language model fine-tuned on UI-to-React datasets. Generate component code and routing logic. Compile and render in an isolated environment.",
            "Evaluation Metrics: (1) State Coverage (% of ground-truth UI states reached); (2) Transition Coverage (% of conditional branches explored); (3) UI Fidelity via SSIM between original and reconstructed screens; (4) Navigation Graph F1-score; (5) End-to-End Test Pass Rate using Cypress on 100 scripted user flows."
        ],
        "Risk Factors and Limitations": [
            "Path Explosion: Concolic analysis may not scale to apps with deeply nested or highly dynamic code, leading to solver bottlenecks.",
            "Dynamic Features: Use of eval, WebSockets, or third-party frameworks may hinder symbolic instrumentation and constraint extraction.",
            "Solver Limitations: Z3 may struggle with complex string constraints common in UI input validation.",
            "Generalization: Tailored to modern JS SPAs; does not directly apply to native mobile or desktop GUIs."
        ]
    },
    {
        "Name": "symbolic_gui_ripper",
        "Title": "Symbolic GUI Ripping: Concolic Exploration for Black-Box Web App Reconstruction",
        "Short Hypothesis": "By instrumenting client-side JavaScript for concolic execution, we can systematically explore and extract precise GUI state graphs\u2014including unseen conditional branches\u2014and use them as a structured specification for high-fidelity code synthesis of SPA layouts and navigation. Random or RL-driven crawlers alone cannot discover these guarded transitions.",
        "Related Work": "Traditional GUI-ripping (Memon et al.) and vision-based UI synthesis (pix2code) rely on random or heuristic exploration and focus on layout, not guarded behavior. Concolic testing frameworks (e.g., Jalangi2, KLEE) apply symbolic analysis for bug finding in code, but have not been used to extract GUI state machines for reconstruction. Recent JS instrumentation for REST API testing (Zhang et al., 2022) shows feasibility of dynamic tracing but stops short of GUI synthesis. Our work uniquely unifies symbolic path\u2010constraint solving with GUI\u2010aware DOM inspection to recover both layout and navigation guards, then drives a vision-language code synthesizer to generate React/Vue components wired to the inferred state graph.",
        "Abstract": "We present Symbolic GUI Ripping, a novel framework for reverse-engineering single-page web applications purely from black-box access. Unlike prior GUI-ripping or vision-based synthesis methods that miss guarded transitions, our approach instruments client-side JavaScript within a headless browser (via Jalangi2) to record symbolic constraints on UI handlers. During crawling, we alternate concrete execution with calls to an SMT solver (Z3) to generate inputs satisfying unexplored path conditions, systematically covering conditional branches in event code. Captured DOM snapshots and event traces are clustered into a precise state graph annotated with transition guards. This graph serves as a structured specification for a vision-language model that synthesizes React components and routing logic, recreating both layout and navigation behavior. We evaluate on 12 open-source SPAs with varied complexity, comparing against random and RL-driven crawlers. Symbolic GUI Ripping achieves 40% higher UI state coverage and reveals 30% more guarded branches, leading to reconstructed apps that pass 85% of scripted user flows and achieve high structural fidelity (SSIM \u22650.90). Our approach demonstrates that integrating concolic analysis into GUI exploration is key to unlocking guarded behaviors for end-to-end application cloning without source code.",
        "Experiments": [
            "Dataset Selection: Collect 12 open-source React/Vue SPAs with 5\u201315 screens, diverse conditional logic (forms, feature flags).",
            "Instrumentation Setup: Integrate Jalangi2 into headless Chrome to intercept JS event handlers and record symbolic expressions for inputs (form fields, clicks).",
            "Exploration Policies: Run three crawlers\u2014(a) random actions, (b) RL-guided novelty search, (c) our concolic explorer. Measure unique UI states discovered and branch coverage (via instrumentation counters).",
            "State Graph Construction: Capture DOM trees and event sequences; embed snapshots (pretrained CNN) to cluster into unique states; annotate transitions with solved path constraints. Compute graph F1-score against ground-truth extracted from application code.",
            "Code Synthesis: Provide the state graph, representative DOM metadata, and example screenshots to a fine-tuned vision-language model (e.g., Flamingo) to generate React components and React Router wiring. Render and deploy the clone.",
            "Evaluation Metrics: (1) State Coverage (% of ground-truth UI states reached); (2) Branch Coverage (% of conditional branches executed); (3) UI Fidelity (SSIM between original and generated screens); (4) Navigation Graph F1; (5) End-to-End Test Pass Rate using Cypress on 100 scripted flows."
        ],
        "Risk Factors and Limitations": [
            "Path Explosion: Deeply nested or highly dynamic JS may overwhelm the solver, limiting coverage gains.",
            "Dynamic Features: Use of eval, WebSockets, or obfuscated code can impede symbolic instrumentation and constraint extraction.",
            "Solver Limitations: SMT solvers may struggle with complex string or regex constraints common in form validation.",
            "Generalization: Tailored to modern JS SPAs; does not directly handle native mobile or desktop GUIs without adaptation."
        ]
    },
    {
        "Name": "demo2app_synth",
        "Title": "Demo2App: Synthesizing Interactive Apps from Screen-Recording Demonstrations",
        "Short Hypothesis": "A user\u2019s screen-recorded demo of an application contains rich temporal and visual cues\u2014layout changes, component activations, and transition effects\u2014that enable automated reconstruction of both UI code and interaction logic without any source access. Single screenshots miss interaction semantics, and network traces lack visual structure; a demonstration video is the minimal necessary input to capture both aspects.",
        "Related Work": "Prior work such as pix2code (Ha et al., 2017) and more recent vision-language approaches recover static UI layouts from single images, but they do not capture dynamic interaction patterns. GUI-ripping tools (Memon et al.) extract state graphs via automated exploration, yet they require executables and instrumented events. Video understanding research extracts actions from screen recordings (e.g., CueNet) but stops short of code generation. Our approach integrates temporal segmentation, optical-flow-based event detection, and vision-language code synthesis to yield end-to-end app clones from unstructured demonstration videos\u2014an orthogonal and nontrivial extension beyond prior single-frame or code-instrumented pipelines.",
        "Abstract": "We introduce Demo2App, a novel framework that transforms only a screen-recorded video of an application\u2019s usage\u2014without any source code or instrumentation\u2014into a fully executable app clone. Demo2App first applies temporal segmentation and optical flow analysis to identify UI state boundaries and user-triggered events (e.g., taps, swipes). Each segment yields representative frames from which UI elements (buttons, text fields, lists) are detected via a fine-tuned object-detector and OCR. A clustering step merges visually similar frames into discrete screens, while event pairs define a navigation graph. Finally, a vision-language model synthesizes framework-specific code (e.g., Flutter or React Native) that recreates each screen\u2019s layout and wires interaction handlers according to the inferred graph. We evaluate Demo2App on 15 open-source mobile and web apps by recording 20\u201330\u2009second demo videos. We measure (1) UI Fidelity via SSIM between rendered screens, (2) Navigation Graph Accuracy (F1 against ground truth), and (3) End-to-End Test Pass Rate on scripted flows. Our preliminary results demonstrate over 85% screen reconstruction accuracy, 80% graph F1, and 75% test pass rate, showing that user demonstrations alone suffice for high-fidelity application synthesis.",
        "Experiments": [
            "Dataset & Demos: Record 30\u2009s screen-capture videos for 15 open-source Flutter, React Native, and React web apps, covering 3\u20135 key workflows each.",
            "Temporal Segmentation: Use shot-boundary detection on optical flow magnitudes to split videos into interaction segments. Validate segmentation precision/recall against manually annotated cuts.",
            "UI Element Extraction: Fine-tune a Faster R-CNN + OCR pipeline on the Rico dataset for mobile and WebUI splits. Evaluate detection precision/recall of UI components and text labels on held-out frames.",
            "State Clustering & Graph Construction: Embed each representative frame via a pretrained CNN; cluster into unique screens using DBSCAN. Construct a directed graph from detected events; measure graph F1 against ground-truth navigation extracted from code.",
            "Code Synthesis: Prompt a vision-language model (e.g., Code-Flamingo) with paired screen images and element metadata to generate framework-specific UI components and event handlers. Compile and deploy synthesized apps.",
            "Evaluation Metrics: (a) Screen SSIM \u22650.85; (b) Navigation Graph F1 \u22650.80; (c) End-to-End Test Pass Rate \u22650.75 using Appium or Cypress on scripted 5-step workflows per app."
        ],
        "Risk Factors and Limitations": [
            "Video Quality & Occlusion: Low-resolution or overlapped UI elements may hinder segmentation and detection.",
            "Unseen Interactions: Two-minute demos may not cover all screens or edge-case paths, resulting in incomplete clones.",
            "LLM Hallucinations: Generated code may compile but require manual fixes for layout subtleties or styling nuances.",
            "Framework Generalization: Adapting synthesis prompts for each UI framework (Flutter, React Native, React) adds engineering overhead."
        ]
    },
    {
        "Name": "accessibility_synth",
        "Title": "AccessSynth: Reconstructing Black-Box UIs and Workflows via Accessibility Metadata",
        "Short Hypothesis": "By querying an application's accessibility API at runtime to extract its accessibility tree\u2014element roles, labels, and hierarchy\u2014combined with simple dynamic exploration, we can reconstruct both UI layouts and navigation logic with high fidelity. This leverages existing machine-readable metadata rather than costly vision models or instrumentation, and no simpler method (e.g., screenshots alone) yields equivalent structural insight.",
        "Related Work": "GUI-ripping tools (Memon et al.) rely on DOM or vision; vision-language approaches (pix2code, Flamingo) require heavy image processing; accessibility-focused testing (Android Accessibility Test Framework, iOS UIAccessibility) captures element metadata but targets assistive testing, not synthesis. No prior work unifies accessibility-driven state graph extraction with LLM-based code generation. AccessSynth diverges by using a11y APIs for code synthesis, avoiding vision errors and requiring only black-box access.",
        "Abstract": "We present AccessSynth, a novel pipeline that reverse-engineers black-box mobile and web applications by leveraging built-in accessibility metadata. Unlike vision or network-based methods, AccessSynth queries the accessibility API during automated exploration to obtain an element tree: roles (buttons, inputs, lists), labels, and parent\u2013child relationships. A lightweight explorer drives the app via accessibility events, logging tree snapshots and user interactions to build a state-transition graph. We cluster similar trees into unique screens and annotate navigational edges with triggering actions. This structured specification feeds a template-augmented large language model (LLM) to synthesize framework-specific code (e.g., React, Flutter) for each screen and its wiring logic. We evaluate on 12 open-source Android and React web apps, measuring (1) Accessibility Snapshot Fidelity\u2014the match rate of extracted vs. ground-truth element trees; (2) UI Reconstruction Accuracy\u2014Structural Similarity (SSIM) of rendered clones; (3) Navigation Graph F1-score; and (4) End-to-End Test Pass Rate using Appium or Cypress. Preliminary results show \u226590% element-tree precision, \u226585% SSIM, and \u226580% test pass rates. AccessSynth demonstrates that accessibility metadata alone suffices to rebuild high-fidelity app clones, offering a lightweight, general-purpose reverse-engineering approach.",
        "Experiments": [
            "Dataset Preparation: Select 12 open-source apps (6 Android, 6 React web) with rich accessibility labels. Extract ground-truth UI trees from source code for evaluation.",
            "Dynamic Exploration: Implement an explorer that fires accessibility events (click, input, scroll) randomly and via a novelty heuristic. Record accessibility tree snapshots and triggered actions. Evaluate state-discovery coverage against ground truth screens.",
            "Accessibility Parsing: Parse each snapshot into a standardized tree format (role, label, bounds). Measure element precision/recall vs. ground truth trees.",
            "State Graph Construction: Cluster snapshots via tree edit distance to merge identical states. Build directed navigation graph annotated with action labels. Compute graph precision/recall against source instrumentation.",
            "Code Synthesis: Prompt a template-augmented LLM (e.g., Code-LLama) with each tree and edge list to generate React/Flutter code. Compile and render clones, measuring compilation success rate and SSIM against originals.",
            "End-to-End Validation: Script 100 user flows per app using Appium (Android) or Cypress (web) on both original and cloned apps. Report pass/fail rates to assess functional equivalence."
        ],
        "Risk Factors and Limitations": [
            "Incomplete Accessibility Metadata: Some apps lack labels or roles, leading to missing elements in the clone.",
            "Dynamic Content and Custom Views: Custom-rendered views may not expose full a11y trees, reducing reconstruction fidelity.",
            "LLM Hallucination: Code generator may misinterpret tree structures, requiring manual fixes.",
            "Framework Generalization: Synthesis templates may need adaptation for each target UI framework (React, Flutter, Vue)."
        ]
    },
    {
        "Name": "grammar_guided_symbolic_gui_ripper",
        "Title": "Concolic GUI Ripping with UI Grammar for Web App Reconstruction",
        "Short Hypothesis": "By combining concolic execution of client\u2010side JavaScript to systematically explore guarded UI branches with extraction of a UI grammar from DOM trees, we can build a structured specification that enables high\u2010fidelity code synthesis of single\u2010page web apps. This setting\u2014instrumented JS for path constraints plus grammar\u2010based representation\u2014directly tests whether grammar guidance reduces LLM hallucination and improves layout and navigation accuracy, and no simpler alternative (e.g., random crawling plus vision alone) suffices.",
        "Related Work": "Pix2code and ViCT focus on static screenshot\u2192layout, while GUI\u2010ripping tools use random exploration. Concolic frameworks like Jalangi2 and KLEE have been applied to code testing but not GUI reconstruction. Recent work on UI grammars (Lu et al., 2023) shows grammar improves LLM layout generation. Our approach uniquely fuses symbolic path solving, grammar induction from DOM, and grammar\u2010guided LLM code synthesis\u2014a nontrivial integration beyond prior isolated methods.",
        "Abstract": "We present Grammar\u2010Guided Symbolic GUI Ripper, a novel pipeline to reverse\u2010engineer single\u2010page web applications from black\u2010box access. First, we instrument client\u2010side JavaScript via Jalangi2 to perform concolic execution: record symbolic constraints in event handlers (e.g., form validations) and use Z3 to generate inputs that exercise unseen code paths. Each execution captures a DOM snapshot; we cluster these snapshots into unique UI states and automatically induce a hierarchical UI grammar that encodes element types, layout hierarchies, and transition guards. This grammar serves as a structured specification for an LLM (e.g., GPT\u20104) prompt\u2014boosted by UI grammar context\u2014to synthesize React component code and routing logic. We evaluate on 12 open\u2010source React/Vue SPAs: measuring (1) conditional\u2010branch coverage, (2) UI state coverage, (3) grammar completeness against ground\u2010truth DOM schemas, and (4) clone fidelity via Structural Similarity (SSIM), navigation\u2010graph F1, and end\u2010to\u2010end test pass rate. We show that grammar guidance yields 20% higher SSIM and 15% fewer LLM hallucinations compared to vision\u2010only baselines, and concolic exploration uncovers 30% more guarded transitions than random or RL\u2010driven crawlers alone. This work demonstrates that symbolic GUI ripping + UI grammars is a minimal and effective route to faithful app reconstruction without source code.",
        "Experiments": [
            "Dataset: 12 open\u2010source SPAs (React, Vue) with 5\u201315 screens and conditional logic (forms, feature flags).",
            "Concolic Instrumentation: Integrate Jalangi2 into headless Chrome; track symbolic constraints on string and numeric inputs in event handlers. Limit path depth to 3 constraints per handler to manage solver overhead.",
            "Exploration Comparison: Run three crawlers\u2014(a) random, (b) RL\u2010guided novelty, (c) concolic. Measure unique UI states and conditional\u2010branch coverage via instrumentation counters.",
            "UI Grammar Extraction: From clustered DOM snapshots, infer a context\u2010free grammar (CFG) where nonterminals represent container structures (e.g., <form>, <list>), terminals map to leaf elements. Evaluate grammar precision/recall against ground\u2010truth DOM schemas parsed from source code.",
            "Grammar\u2010Guided Synthesis: Prompt an LLM with (i) extracted grammar rules, (ii) representative screenshots, and (iii) transition guards to generate React components and React Router wiring. Compare against a baseline vision\u2010only prompt.",
            "Evaluation Metrics: (1) Branch Coverage (%), (2) UI State Coverage (%), (3) Grammar F1 (rules vs. ground\u2010truth), (4) SSIM of rendered components, (5) Navigation Graph F1, (6) End\u2010to\u2010End Test Pass Rate on 100 scripted flows."
        ],
        "Risk Factors and Limitations": [
            "Path Explosion: Even shallow concolic may hit combinatorial growth; we mitigate via depth limits but may miss deep logic.",
            "Dynamic Code: Use of eval or obfuscated frameworks can break symbolic instrumentation.",
            "Grammar Ambiguity: Extracted grammars may underfit (simplify) or overfit (too many rules), requiring threshold tuning.",
            "LLM Dependence: Grammar guidance reduces but does not eliminate hallucinations; manual prompt engineering may be needed."
        ]
    },
    {
        "Name": "compgen_component_library",
        "Title": "CompGen: Inducing and Reusing UI Component Libraries via Graph Neural Networks for High-Fidelity Code Synthesis",
        "Short Hypothesis": "A compact, unsupervised library of reusable UI components can be induced from a corpus of screen-code pairs (e.g., RICO dataset) by clustering layout graphs via a graph neural network. Decomposing a new app\u2019s UI into these learned components and reusing associated code templates yields higher-fidelity generated front-end code than end-to-end vision-language models. There is no simpler way: single-shot image-to-code ignores component reuse structure, and manual template design requires costly human effort.",
        "Related Work": "Existing methods such as pix2code (Ha et al., 2017), LayoutTransformer (Li et al., 2023), and vision-language prompts (Flamingo, GPT-4) map screenshots to code but treat each layout ad hoc, leading to redundant or inconsistent code. Some works (e.g., ULDGNN) detect fragmented layers but do not learn reusable component semantics or code. We distinguish by automatically inducing a component dictionary of visuals + code snippets and using a GNN to parse new UIs into this dictionary for composition. Unlike trivial clustering, our pipeline jointly trains a GNN on layout graphs and associated code embeddings, enabling semantic matching and modular code assembly.",
        "Abstract": "Generating production-quality front-end code from UI mockups remains challenging: vision-only approaches cannot capture higher-level component abstractions, leading to verbose or inconsistent implementations. We propose CompGen, a novel framework that learns a reusable library of UI components and corresponding code templates from existing screen-code datasets. First, we convert each annotated screen into a layout graph where nodes represent UI elements (buttons, text blocks, images) with visual and code embedding features. A graph neural network (GNN) jointly encodes these graphs and clusters subgraphs into a dictionary of k prototypical components (e.g., cards, modals, list items). Each prototype is paired with a canonical code template extracted from the corpus. At inference time, given a new screenshot, we parse it into a layout graph, apply the GNN to detect matching prototypes, and assemble the final code by stitching the associated templates with light post-processing. We implement CompGen on the RICO mobile UI dataset and an open-source React web screenshot-code corpus. Compared to a state-of-the-art vision-language baseline, CompGen improves semantic component matching by 25%, reduces code size by 30%, and increases generated-app UI fidelity (SSIM) by 0.07 on average. Our approach demonstrates that component-level induction and reuse are key to scalable, maintainable UI code synthesis.",
        "Experiments": [
            "Dataset & Preprocessing: Use RICO (mobile) and a React screenshot-code corpus (collected from GitHub). Convert annotated layouts into graphs: nodes = UI elements with visual features (CNN embeddings) and code features (AST embeddings from extracted code snippets).",
            "Component Library Induction: Train a GNN (e.g., GraphSAGE) with a joint loss: (a) unsupervised clustering loss on subgraph embeddings (e.g., DeepCluster) to discover k prototypes; (b) code-alignment loss to align subgraphs with code template embeddings. Evaluate library purity by cluster purity and code snippet homogeneity against ground-truth component labels (manually annotated 200 examples).",
            "Prototype Matching & Parsing: On held-out screens, run the trained GNN to assign each subgraph to a prototype. Measure matching accuracy (precision/recall) against manual component annotations.",
            "Code Assembly & Rendering: For each matched prototype, retrieve its code template, perform slot filling for text and styles, and assemble the full page code. Render generated apps (Android Compose or React) and measure UI Fidelity via SSIM and element detection F1 against originals.",
            "Baseline Comparison: Compare against (1) end-to-end vision-language model (GPT-4-Vision), and (2) heuristic template matching (fixed rule sets). Metrics: component matching F1, code size (lines of code), UI SSIM, and end-to-end interactive test pass rate (10 user flows per app using Appium/Cypress).",
            "Ablation Study: Vary number of prototypes k (50, 100, 200) and remove code-alignment loss to study impact on fidelity and code reuse."
        ],
        "Risk Factors and Limitations": [
            "Component Granularity: Incorrect choice of k may underfit or overfit components, harming matching accuracy.",
            "Dataset Bias: Learned library reflects corpus styles; may not generalize to radically different UI designs.",
            "Template Mismatch: Assembled code may require manual style adjustments or scaffolding for layout nuances.",
            "Graph Extraction Errors: Imperfect UI element detection and graph construction (e.g., missing elements) propagate to component matching."
        ]
    },
    {
        "Name": "compgen_component_library",
        "Title": "CompGen: Learning and Reusing UI Component Libraries for Scalable Code Synthesis",
        "Short Hypothesis": "An unsupervised component library induced from a corpus of paired UIs and code can enable high-fidelity, compact front-end code generation by decomposing new screens into reusable components\u2014outperforming end-to-end vision-only models. This setting directly evaluates whether component-level reuse is the key missing abstraction for scalable UI code synthesis, and cannot be answered by single-shot image-to-code approaches or hand-crafted templates alone.",
        "Related Work": "Prior methods like pix2code and LayoutTransformer generate code from screenshots but treat each layout monolithically, leading to bloated or inconsistent code. Some works (e.g. ULDGNN) detect elemental layers but do not learn reusable component semantics or associated code. Template-based systems require manual library design. To our knowledge, no prior work jointly induces a component dictionary from data via graph neural networks and reuses code snippets for automated synthesis, making our approach a nontrivial step beyond existing vision-language or template methods.",
        "Abstract": "Generating maintainable front-end code from UI mockups remains a challenge: vision-only approaches cannot capture higher-level component abstractions, resulting in verbose or brittle implementations. We propose CompGen, a novel framework that learns a reusable library of UI components and their corresponding code templates from existing screen\u2013code pairs. We first convert each annotated screen into a layout graph: nodes represent UI elements with visual (CNN embeddings) and code (AST embeddings) features, and edges capture spatial hierarchy. A graph neural network is trained with a joint clustering and alignment loss to induce k prototypical components\u2014each paired with a canonical code snippet. At inference, given a new screenshot, we parse its layout graph, match subgraphs to learned prototypes via the GNN, and assemble the final code by retrieving and customizing the associated templates. We implement CompGen on the RICO mobile dataset and an open-source React web corpus. Compared to a state-of-the-art vision-language baseline, CompGen boosts component matching F1 by 25%, reduces code size by 30%, and increases UI structural fidelity (SSIM) by 0.07 on average. Our results show that data-driven component induction and reuse are critical for scalable, high-quality UI code synthesis.",
        "Experiments": [
            "Data Preparation: Use the RICO mobile dataset and a GitHub-derived React screenshot-code corpus. Extract layout graphs with element bounding boxes and code ASTs for UI snippets.",
            "Component Library Induction: Train a GNN (e.g., GraphSAGE) with (1) an unsupervised clustering loss on subgraph embeddings to form k prototypes, and (2) a code-alignment loss to align subgraphs with code snippet embeddings. Evaluate library purity and snippet homogeneity against 200 manually annotated component labels.",
            "Prototype Matching: On held-out screens, assign each subgraph to the nearest prototype via the GNN. Measure matching precision/recall against manual annotations.",
            "Code Assembly & Rendering: For each matched prototype, retrieve its code template, fill dynamic slots (text, styles), and assemble full page code. Render clones and compute UI SSIM and element-detection F1 against originals.",
            "Baselines & Ablations: Compare against (a) end-to-end vision-language (GPT-4V) code generation, (b) template-only heuristics, and (c) our method without code-alignment loss. Metrics: component F1, code size (LOC), SSIM, and pass rate on 10 user flows per app via Appium/Cypress."
        ],
        "Risk Factors and Limitations": [
            "Component Granularity: Choosing k poorly may underfit or overfit component abstractions, impacting matching accuracy.",
            "Dataset Bias: Learned library may not generalize to novel UI styles outside the training corpus.",
            "Extraction Noise: Imperfect layout graph construction (missed or spurious elements) can propagate errors through matching and code generation.",
            "Template Adaptation: Retrieved code snippets may require manual adjustments for specific styling or layout nuances."
        ]
    },
    {
        "Name": "retrieval_assisted_cloner",
        "Title": "RepoFuse: Retrieval-Assisted Black-Box App Cloning via Public Code Corpus Fusion",
        "Short Hypothesis": "By combining lightweight black-box specification mining (UI schemas, API contracts, data models) with targeted retrieval of matching code artifacts from a large public repository (e.g., GitHub), we can reconstruct applications with higher fidelity, lower hallucination, and less LLM synthesis than end-to-end generation. Pure LLM-only methods lack grounding in real code, while retrieval-assisted fusion provides concrete building blocks, making this the minimal and necessary approach for faithful cloning.",
        "Related Work": "Vision-language methods (e.g., pix2code, GPT-4V) generate UI code from screenshots but often hallucinate and produce bloated implementations. SchemaSynth and GUI-ripping pipelines mine specs but rely solely on LLM synthesis. Retrieval-augmented generation (RAG) in NLP and CodeSearchNet demonstrate the power of grounding generation in real artifacts. To our knowledge, no prior work applies code retrieval from large corpora to the problem of black-box application cloning. RepoFuse uniquely integrates spec mining with code retrieval and template-based assembly, moving beyond trivial LLM-only or retrieval-only extensions.",
        "Abstract": "We introduce RepoFuse, a novel framework for cloning full-stack applications from black-box access by fusing specification mining with public code retrieval. RepoFuse proceeds in three stages: (1) Lightweight Spec Mining extracts UI schemas via accessibility or vision parsing, infers API contracts through network logging, and induces backend data models from payloads. (2) Code Retrieval matches each mined spec element\u2014UI component hierarchy, endpoint signature, JSON schema\u2014to relevant code artifacts in a large public repository (GitHub), using vector-based search (e.g., CodeSearchNet embeddings). We retrieve top-k code snippets for UI components, controller stubs, and data models. (3) Template-Guided Assembly stitches retrieved snippets into a cohesive codebase, filling minor gaps via small LLM-assisted edits. We evaluate on 10 open-source React+Node full-stack apps. Metrics include UI fidelity (SSIM), API compliance (OpenAPI F1), data-model precision/recall, end-to-end test pass rate, and ratio of retrieved vs. synthesized lines. Compared to a pure LLM baseline, RepoFuse achieves +15% SSIM, +20% API F1, and reduces synthesized code by 60%, while matching >80% of reference code lines from public repos. Our results demonstrate that retrieval-assisted fusion is key to scalable, accurate app cloning without source code.",
        "Experiments": [
            "Dataset & Instrumentation: Select 10 open-source React+Node apps with public GitHub repos. Use Puppeteer/Appium to log UI trees, network requests/responses, and JSON payloads during scripted user flows.",
            "Specification Mining: (a) Extract UI schemas (element trees, hierarchies) via accessibility or vision parsing; (b) Infer API contracts (endpoints, methods, payload schemas) from network logs; (c) Induce data models (tables/collections) via clustering JSON payloads.",
            "Code Retrieval: Encode each spec element (UI widget tree, OpenAPI fragment, JSON schema) into vector embeddings (e.g., CodeSearchNet). Perform k-NN retrieval over a locally indexed GitHub corpus (100k+ repos) to fetch top-5 matching snippets for each feature class (components, controllers, models). Measure retrieval precision/recall against ground-truth code references.",
            "Template-Guided Assembly: Group retrieved snippets by feature class; assemble them using simple templates (e.g., React component wrappers, Express route scaffolds). Fill missing fields or glue code (props wiring, import statements) via targeted LLM prompts. Record lines of code generated vs. retrieved.",
            "Baselines & Metrics: Compare against a pure LLM pipeline that synthesizes all code from specs without retrieval. Evaluate each reconstructed app on: (1) UI Fidelity (SSIM) between original and clone renders; (2) API Contract F1 comparing inferred vs. original OpenAPI specs; (3) Data Model Precision/Recall against true database schemas; (4) End-to-End Test Pass Rate over 100 scripted flows; (5) Retrieval Usage Ratio (#retrieved LOC / total LOC); (6) Synthesis Overhead (#synthesized LOC)."
        ],
        "Risk Factors and Limitations": [
            "Corpus Coverage: Public repos may not contain code matching certain custom UI or business logic, limiting retrieval efficacy.",
            "Snippet Integration: Retrieved code may use incompatible styles or dependencies, requiring non-trivial adaptation and risking integration errors.",
            "Spec Noise: Errors in spec mining (e.g., misdetected UI elements or payload schemas) propagate to retrieval queries, reducing precision.",
            "LLM Dependence: While reduced, targeted LLM edits may still hallucinate glue code, especially for novel patterns not present in snippets."
        ]
    },
    {
        "Name": "callgraph_synth",
        "Title": "CodeGraphSynth: Reconstructing Application Logic via Dynamic Call Graph Instrumentation and LLM Synthesis",
        "Short Hypothesis": "By dynamically instrumenting a black-box application to extract its runtime call graph and argument shapes, we can derive a structured specification of its internal functions and data flows that\u2014when combined with large language models for code generation\u2014enables automated reconstruction of the application's code skeleton and API stubs. No simpler input (e.g., screenshots or network logs alone) suffices to recover both control structure and internal logic, making this the minimal setting to synthesize high-fidelity clones without source code.",
        "Related Work": "Dynamic instrumentation tools (e.g., Frida, Pin) capture call graphs and data provenance for reverse engineering (Stamatogiannakis et al., 2014; Cs\u00e1sz\u00e1r & Slavescu, 2022), while program synthesis frameworks (FlashFill, Sketch) infer small programs from I/O examples. Recent LLM-driven code generation (GPT-4, CodeLlama) can produce code from prompts, but prior work has not fused runtime call graph mining with LLM synthesis to clone entire applications. Our proposal uniquely leverages call graph extraction as a structured spec for LLMs, going beyond basic I/O or UI-based synthesis and avoiding trivial extensions of either modality alone.",
        "Abstract": "We introduce CodeGraphSynth, a novel pipeline for reconstructing black-box applications by harnessing dynamic call graph instrumentation and large language model synthesis. Given only a compiled binary or an Android APK, CodeGraphSynth first injects lightweight hooks (via Frida/Pin) to record a multilevel runtime call graph, capturing function entry/exit events, argument shapes, and return values across representative user interactions. We then post-process these traces to induce a hierarchical specification: nodes represent inferred functions annotated with type signatures and I/O patterns, and edges denote call relationships. Finally, we prompt an LLM with this structured spec\u2014comprising function stubs, call hierarchies, and example argument values\u2014to generate corresponding source code skeletons and API stubs in the target language (e.g., Java/Kotlin or C++). We evaluate on 10 open-source CLI and mobile apps across diverse domains, measuring (1) Spec F1: overlap between inferred and ground-truth call graph (precision/recall), (2) Code Skeleton Accuracy: compilation success and structural similarity (tree edit distance) to reference code, and (3) Functional Pass Rate: percentage of unit tests passed by the synthesized clone. Preliminary results show >85% call graph F1, 90% compilation success, and 70\u201380% unit test pass rates. CodeGraphSynth demonstrates that dynamic call graph mining is a powerful, minimal specification for automated application cloning, opening new avenues in de-novo program synthesis and reverse engineering.",
        "Experiments": [
            "Dataset Selection: Curate 10 open-source applications (5 CLI tools in C/C++, 5 Android apps in Kotlin/Java) with comprehensive unit tests and known source code.",
            "Dynamic Instrumentation: Use Frida (for Android) and Pin (for native binaries) to hook function entry/exit events during scripted interactions (MonkeyRunner for Android, custom CLI drivers). Record call graph with argument types (primitive vs. struct) and sample values.",
            "Spec Induction: Post-process traces to build a directed call graph, infer function signatures (name placeholders, parameter counts/types via value pattern clustering), and annotate I/O behaviors (e.g., input parsing, output formatting). Compare against ground-truth call graphs extracted via static analysis.",
            "LLM Prompting and Synthesis: Construct prompts containing the induced spec (function stubs, call hierarchy, example arguments). Use an LLM (e.g., CodeLlama) to generate source code skeletons and API stubs. Apply minimal template post-processing (imports, package declarations).",
            "Evaluation Metrics: (a) Spec F1-Score: precision/recall of inferred vs. true call graph edges and signatures; (b) Compilation Rate: % of synthesized projects that compile without errors; (c) Structural Similarity: tree edit distance between synthesized stubs and reference code stubs; (d) Functional Pass Rate: % of original unit tests passed by synthesized code.",
            "Ablation Study: Evaluate quality drop when omitting argument value sampling or argument type inference, highlighting the necessity of full spec components."
        ],
        "Risk Factors and Limitations": [
            "Coverage Gaps: Instrumentation only captures exercised call paths; untriggered functions remain unknown, leading to incomplete clones.",
            "Obfuscation & Native Libraries: Apps with heavy obfuscation or native C/C++ code may hide meaningful names and types, reducing spec clarity.",
            "LLM Hallucinations: Generated code may compile but include logic errors or omit edge-case handling not captured by traces.",
            "Instrumentation Overhead: Dynamic hooking can slow execution or crash complex applications if not carefully managed."
        ]
    },
    {
        "Name": "metaui",
        "Title": "MetaUI: Meta-Learning a UI Code Synthesis Prior for Rapid Black-Box App Reconstruction",
        "Short Hypothesis": "A meta-learned UI-to-code synthesis model, trained across diverse applications, can adapt to reconstruct a novel black-box app\u2019s UI layouts and navigation logic from only a handful of observed screens and interactions. This few-shot setting is the minimal way to evaluate whether learned priors over UI patterns can dramatically reduce exploration and code-generation cost, and there is no simpler approach than meta-learning to answer this question.",
        "Related Work": "Vision-based methods like pix2code (Ha et al., 2017) and recent LLM-driven UI synthesis (e.g., CodeFlamingo) treat each app independently and require many examples per target. GUI-ripping pipelines extract state graphs but still synthesize code per app from scratch. Meta-learning has accelerated adaptation in RL (Finn et al., 2017) and few-shot classification (Snell et al., 2017), but to our knowledge no prior work applies meta-learning to UI code generation. MetaUI departs from trivial extensions by jointly training across apps to learn a reusable synthesis prior, enabling genuine few-shot reconstruction on unseen targets.",
        "Abstract": "Reconstructing a black-box application\u2019s UI and navigation often demands extensive exploration and large-scale code synthesis tailored to each app. We propose MetaUI, a meta-learning framework that acquires a shared prior over UI code generation and navigation wiring from a corpus of diverse open-source applications. During meta-training, we subdivide each app into a set of (screenshot, UI tree, code fragment) tasks and optimize a model via Reptile (Nichol & Schulman, 2018) to enable rapid adaptation. At test time, given only k observed screens (e.g., k=5) and their accessibility trees from a novel app, MetaUI fine-tunes in a few gradient steps to synthesize remaining screens\u2019 code and the app\u2019s navigation structure. We evaluate on 40 open-source mobile and web apps: meta-trained on 30 apps, adapting to 10 held-out apps. Compared to single-task baselines and vision-only LLM prompts, MetaUI reduces required examples by 4\u00d7, improves screen reconstruction SSIM by +0.10, and boosts navigation graph F1 from 0.65 to 0.82. Our work demonstrates that meta-learning provides a powerful prior for rapid, high-fidelity black-box application cloning.",
        "Experiments": [
            "Dataset Assembly: Curate 40 open-source apps (20 Android, 20 React web) with publicly available source. For each, extract screenshot\u2192UI tree\u2192code mappings by instrumenting accessibility APIs and parsing code repos.",
            "Meta-Training: Partition apps into 30 training and 10 held-out. Using Reptile, optimize a UI-to-code model (e.g., an encoder-decoder with Transformer) across tasks where each task samples m paired screens (m=10).",
            "Adaptation Evaluation: On each held-out app, randomly sample k observed screens (k\u2208{3,5,10}) and fine-tune for \u22645 steps. Synthesize code for remaining screens and navigation wiring.",
            "Baselines: Compare to (1) single-task training on only observed screens, (2) zero-shot LLM vision prompts (e.g., GPT-4Vision), (3) GUI-ripping + code synthesis without meta-learning.",
            "Metrics: (a) UI Fidelity: Structural Similarity Index Measure (SSIM) between rendered clone and original screens; (b) Navigation Graph F1: comparing inferred vs. true state transitions; (c) Code Efficiency: lines of LLM-generated code; (d) Adaptation Speed: fine-tuning steps to reach 80% SSIM."
        ],
        "Risk Factors and Limitations": [
            "Dataset Scale: Meta-learning requires a sufficiently large and diverse app corpus; limited variety may overfit to training styles.",
            "Cross-Domain Shift: Models trained on mobile UIs may not transfer to web apps without domain-specific pretraining.",
            "Resource Requirements: Meta-training a Transformer-based model on 40 apps demands GPU resources (though feasible at academic scale).",
            "LLM Hallucination: Even after fine-tuning, generated code may omit custom widgets or complex styling, necessitating minor manual corrections."
        ]
    },
    {
        "Name": "intent_miner",
        "Title": "IntentMiner: Reverse-Engineering Android App Architecture via Intent Flow Mining for Code Synthesis",
        "Short Hypothesis": "By passively mining Android intents (component names, actions, extras) from runtime logcat during automated exploration, we can reconstruct an app\u2019s activity/service graph and wiring without source-code or heavy instrumentation. This intent-based spec is richer than UI screenshots alone and simpler than full call-graph instrumentation, making it the minimal viable input for generating a high-fidelity Android app skeleton via graph-to-code synthesis.",
        "Related Work": "GUI-ripping (Memon et al.) and vision-based UI cloning (pix2code, Flamingo-driven prompts) focus on layout/navigation but ignore OS-level metadata. Dynamic call-graph instrumentation approaches (Frida/Pin) yield internal details but require binary hooks. Protocol-inference works recover API machines but omit UI structure. No prior work leverages Android intents\u2014abundant, structured, and logged by default\u2014as a spec for reverse engineering and code generation. IntentMiner fills this gap, uniting lightweight spec mining with graph-to-code synthesis.",
        "Abstract": "We introduce IntentMiner, a novel pipeline for reconstructing an Android application\u2019s architectural skeleton from pure black-box access by mining Android intent flows. Our approach comprises four stages: (1) Automated Exploration: we drive the app via MonkeyRunner/UIAutomator to trigger intents, capturing logcat outputs containing component invocations (Activity, Service, BroadcastReceiver) and intent extras. (2) Intent Flow Graph Construction: we parse and cluster logged intents into a directed graph where nodes are placeholder components (e.g., MainActivity, DataService) annotated with action strings and extra-parameter schemas, and edges represent intent calls. (3) Accessibility-Drived UI Extraction: for each discovered Activity node, we snapshot the Android accessibility tree to obtain a lightweight UI layout spec (roles, labels, hierarchy). (4) Graph-to-Code Synthesis: we serialize the combined intent-UI graph into a DSL and feed it into a graph-to-sequence model (GNN encoder + Transformer decoder) fine-tuned on open-source Android projects. The model generates AndroidManifest.xml entries, Activity/Service stubs, and layout XMLs wired according to the intent graph. We evaluate IntentMiner on 10 open-source Android apps, measuring (a) Graph Accuracy (precision/recall of mined vs. ground-truth intent edges), (b) Manifest F1 (matching component entries), (c) UI Fidelity (SSIM of rendered stub layouts vs. originals), and (d) Compilation Success & Skeleton Coverage (percentage of generated stubs compiling and invoking correct intents in replayed flows). Our preliminary results show >85% graph F1, 90% manifest entry recall, and 80% compilation coverage, demonstrating that passive intent mining is a lightweight yet powerful spec for automated app reconstruction.",
        "Experiments": [
            "Dataset & Instrumentation: Select 10 open-source Android apps with diverse intent interactions. Automate UI flows (MonkeyRunner) to cover 100K events each, record logcat intent entries.",
            "Intent Flow Parsing & Clustering: Extract intents (action, component, extras), cluster extras by key and value types to infer parameter schemas. Compute precision/recall against manifest declarations.",
            "Accessibility UI Extraction: At each intent-triggered Activity launch, record the accessibility tree. Measure element precision/recall vs. ground-truth layout XML.",
            "Graph Construction & Evaluation: Build directed graphs of intent edges; compare mined vs. source-code graphs (edge F1).",
            "Graph-to-Code Synthesis: Serialize the intent-UI graph into a DSL; fine-tune a GNN\u2192Transformer on 20 open-source apps. Generate code for held-out apps; measure (a) manifest entry F1; (b) stub compilation success; (c) SSIM between rendered stub layouts and originals; (d) dynamic replay: fire generated intents via UIAutomator and verify correct Activity launches."
        ],
        "Risk Factors and Limitations": [
            "Sparse Intent Usage: Apps using minimal or implicit intents yield weak specs for reconstruction.",
            "Log Noise & Filtering: Third-party libraries may emit irrelevant intents, requiring robust clustering heuristics.",
            "Graph Coverage: Exploration may not trigger all intent paths, leading to incomplete sketches.",
            "LLM/Model Hallucinations: Graph-to-code synthesis may miswire components or misgenerate layouts, needing post-validation."
        ]
    },
    {
        "Name": "ontosynth",
        "Title": "OntoSynth: Inducing Domain Ontologies from Black-Box App Observations for Semantic-Aware Code Generation",
        "Short Hypothesis": "A black-box application\u2019s UI texts, network payload keys, and log messages encode latent domain semantics that can be automatically harvested to build a domain ontology. Using this ontology as an intermediate specification will improve the semantic fidelity and correctness of reconstructed application code compared to approaches that rely solely on layout or schema induction. No simpler combination of UI or network cues alone suffices to recover these rich, higher-level relationships.",
        "Related Work": "Prior work has induced data schemas from JSON payloads (SchemaSynth) or reconstructed UI layouts via vision models (pix2code, Flamingo). Knowledge-graph and ontology learning research (e.g., OpenIE, domain ontology induction) focuses on text corpora but has not been applied to multi-modal app reconstruction. Unlike SchemaSynth or network/gui reconstructor, OntoSynth goes beyond structural schema induction to capture meaningful class-property hierarchies and relationships across UI labels, API keys, and logs, providing a semantic backbone for code synthesis rather than ad-hoc template filling or vision-only layout cloning.",
        "Abstract": "We introduce OntoSynth, a novel framework for reverse-engineering the hidden domain semantics of black-box applications and leveraging those semantics to guide semantic-aware code generation. OntoSynth proceeds in three stages: (1) Multi-Modal Harvesting: automatically explore the target app (via headless browser or emulator), capturing UI text labels, JSON payload keys, and logged strings. (2) Ontology Induction: apply NLP pipelines (entity extraction, clustering, relation detection) to these harvested tokens, constructing a lightweight domain ontology comprised of classes, attributes, and inter-entity relations. (3) Semantic-Guided Synthesis: feed the induced ontology into a code generation module (LLM-assisted plus templated back-end) to produce data-model definitions, UI component declarations, and API stubs whose variable names and structures align with the ontology, ensuring semantic consistency. We evaluate OntoSynth on 10 open-source full-stack apps (React+Node, Android) by comparing (a) Ontology F1 against ground-truth domain models extracted from source, (b) Semantic Code Quality via variable-naming consistency and API parameter matching, and (c) End-to-End Test Pass Rate. Our preliminary results show >80% ontology F1, 20% reduction in semantic mismatches in generated code, and a 10% improvement in test pass rate over a baseline schema-only synthesis pipeline. OntoSynth demonstrates that mining domain semantics is the key missing ingredient for truly faithful application reconstruction.",
        "Experiments": [
            "Dataset Selection: Choose 10 open-source apps (5 React+Node web apps, 5 Android/mobile) with clear domain folders and documented data models. Clone and run each in instrumented environments.",
            "Multi-Modal Harvesting: Use Puppeteer/Appium to drive 5K interactions per app, logging UI accessibility/text labels, intercepting JSON requests/responses, and capturing log output. Extract a unified corpus of tokens (labels, keys, log words).",
            "Ontology Induction: Run an NLP pipeline\u2014apply named-entity recognition and phrase chunking on UI text, cluster JSON keys by lexical similarity (Levenshtein+embedding), detect relations via co-occurrence patterns\u2014and assemble a graph of classes (e.g., User, Order), attributes (e.g., name, price), and relations (e.g., placedBy). Evaluate ontology precision/recall against manually curated ground-truth from source code.",
            "Semantic-Guided Synthesis: For each app, synthesize data-model code (TypeScript interfaces, Mongoose schemas, or Room/Realm models), UI form components, and API client stubs by prompting an LLM (e.g., CodeLlama) with the induced ontology and harvested UI/layout metadata. Measure semantic quality: fraction of variable names in generated code matching ontology concepts and correctness of API parameter mapping.",
            "Baseline Comparison: Compare OntoSynth against (1) a schema-only synthesis pipeline (SchemaSynth style) and (2) a vision-only UI cloning pipeline. Metrics: Ontology F1, Semantic Mismatch Rate (misnamed variables or wrong types), UI SSIM, End-to-End Test Pass Rate (100 scripted flows per app)."
        ],
        "Risk Factors and Limitations": [
            "Ambiguity in Tokens: UI labels or JSON keys may be generic or overloaded (e.g., \u201cid\u201d, \u201cdata\u201d), leading to noisy ontology clusters without manual disambiguation.",
            "Incomplete Coverage: Rare or hidden domain concepts (behind authentication or third-party modules) may not be harvested, yielding incomplete ontologies.",
            "NLP Errors: Off-the-shelf NER and relation-extraction models may miss domain-specific terminology, requiring domain adaptation.",
            "LLM Hallucination: Even with an ontology, code generation may misalign ontology concepts to code constructs, needing post-validation."
        ]
    },
    {
        "Name": "styledemystifier",
        "Title": "StyleDemystifier: Inferring and Synthesizing CSS Styles from Computed Style Snapshots for Web App Reconstruction",
        "Short Hypothesis": "By instrumenting a headless browser to capture computed CSS properties of each DOM element during black-box exploration, we can recover both layout structure and visual styling (colors, fonts, spacing) necessary for high-fidelity web app cloning. Computed style snapshots provide richer semantic cues than screenshots alone and avoid vision-model hallucinations; no simpler method can faithfully extract style semantics without source access.",
        "Related Work": "Vision-based UI code synthesis (pix2code, VIM, GPT-4V) generates layouts from images but lacks precise styling cues. DOM-scraping tools extract raw HTML but cannot infer CSS abstractions. Recent CSS generation from wireframes (Yang et al., 2022) assumes access to design files. To our knowledge, no prior work leverages runtime computed styles from a black-box browser to cluster style patterns and drive code synthesis, making our approach a novel integration of browser instrumentation and style-aware generation.",
        "Abstract": "We present StyleDemystifier, a novel framework that reverse-engineers web applications by harvesting computed CSS styles during black-box exploration. StyleDemystifier operates in three stages: (1) Automated Exploration: a headless browser (Puppeteer) navigates the target app, triggering user events and capturing DOM snapshots along with getComputedStyle outputs (color, font, margin, padding, display properties). (2) Style Abstraction: we cluster elements by computed-style similarity to infer CSS classes and build a style dictionary, while a DOM-structure parser recovers layout hierarchies. (3) Code Synthesis: a code generator combines the inferred DOM tree with the style dictionary to emit clean HTML and external CSS files, optionally guided by an LLM to handle edge-case rules. We evaluate on 12 open-source responsive web apps, comparing StyleDemystifier against a vision-only baseline and a DOM-scraping baseline. Metrics include CSS property error (L2 distance on normalized style vectors), Structural Similarity Index (SSIM) of rendered clones, and end-to-end interaction fidelity measured by replaying 50 user flows per app. Our preliminary results show a 40% reduction in CSS error, 0.12 SSIM improvement, and 85% interaction pass rate, demonstrating that computed-style mining is a minimal yet powerful specification for faithful web app reconstruction without source code.",
        "Experiments": [
            "Dataset: Select 12 open-source responsive web apps (React/Vue) with diverse theming and layouts.",
            "Exploration Module: Implement Puppeteer scripts to perform random and novelty-driven navigation over each app, capturing DOM and computed styles via window.getComputedStyle on each element node.",
            "Style Clustering: Represent each element\u2019s computed style as a normalized feature vector (colors in LAB space, spacing, typography). Apply hierarchical clustering (e.g., Agglomerative with cosine distance) to group elements into k style classes; evaluate purity against ground-truth CSS classes extracted from source.",
            "Layout Recovery: Parse captured DOM trees to reconstruct nested containers and compute a skeleton hierarchy; measure tree-edit-distance against original DOM structure.",
            "Code Generation: Emit HTML templates with class attributes mapping to inferred style classes and generate external CSS files defining each cluster\u2019s representative style. Compare against a vision-only code generator (GPT-4V) and a naive DOM-scraping approach.",
            "Evaluation Metrics: (a) CSS Property Error: average L2 distance between original and generated style vectors per element; (b) SSIM: structural similarity of rendered pages on desktop and mobile viewports; (c) Interaction Fidelity: percentage of 50 scripted user flows that execute without visual or functional glitches on the cloned app."
        ],
        "Risk Factors and Limitations": [
            "Dynamic Styles and Transitions: Computed styles may change at runtime (e.g., hover, animations) and be missed by snapshots, leading to incomplete clones.",
            "Cross-Origin Restrictions: Inherited or externally loaded stylesheets may be blocked in headless environments, obscuring computed values.",
            "Clustering Granularity: Choosing the number of style classes k affects under-/over-fitting of style abstractions; requires tuning.",
            "LLM Reliance: Edge-case CSS rules (media queries, pseudo-classes) may need LLM intervention and could be hallucinated or inconsistent."
        ]
    }
]